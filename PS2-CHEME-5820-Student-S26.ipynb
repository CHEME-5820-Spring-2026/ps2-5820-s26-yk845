{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2017de7-07b5-4d18-ba73-17e2d7767409",
   "metadata": {},
   "source": [
    "<a id=\"introduction\"></a>\n",
    "\n",
    "# PS2: Logistic Regression Classification of a Clinical Dataset\n",
    "Can simple linear classifiers predict whether a heart-failure patient will survive based on routine clinical measurements? In this problem set, we address that question by training a Perceptron and a Logistic Regression classifier on a real clinical dataset of 299 patients.\n",
    "\n",
    "We preprocess the data, visualize it with PCA, train both classifiers on the same train/test split, and compare their performance using confusion-matrix metrics. Work through the notebook, correct any issues, and answer the discussion questions at the end.\n",
    "\n",
    "> **Learning Objectives:**\n",
    "> \n",
    "> By the end of this problem set, you should be able to:\n",
    "> \n",
    "> * **Preprocess clinical data for classification:** Recode binary features to $\\{-1,1\\}$, apply z-score normalization to continuous features, and split into training and test sets.\n",
    "> * **Use PCA to visualize high-dimensional patient data:** Compute the covariance matrix, perform eigendecomposition, and project onto the top two principal components to assess class separability.\n",
    "> * **Train, evaluate, and compare linear classifiers:** Fit perceptron and logistic regression models and assess accuracy, precision, and recall on the test set using confusion matrices.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ee3ba1-1b62-41f8-9d0f-075854147d4f",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "\n",
    "## Setup, Data, and Prerequisites\n",
    "We set up the computational environment by including the `Include.jl` file. The `Include.jl` file loads external packages, various functions that we will use in the exercise, and custom types to model the components of our lab problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af6d93f-875f-4a42-b5b4-854eef732a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b460c4-34db-4b40-88b5-80396ff6c0fb",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>\n",
    "\n",
    "### Data\n",
    "Next, let's load up the dataset that we will explore. The data for this lab was taken from this `2020` publication:\n",
    "* [Davide Chicco, Giuseppe Jurman: \"Machine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone.\" BMC Medical Informatics and Decision Making 20, 16 (2020). https://doi.org/10.1186/s12911-020-1023-5](https://pubmed.ncbi.nlm.nih.gov/32013925/)\n",
    "\n",
    "In this paper, the authors analyzed a dataset of 299 heart failure patients collected in 2015. The patients comprised 105 women and 194 men, aged between 40 and 95 years old. The dataset contains 13 features (a mixture of continuous and categorical data), which report clinical, body, and lifestyle information:\n",
    "* Some features are binary: anemia, high blood pressure, diabetes, sex, and smoking status.\n",
    "* The remaining features were continuous biochemical measurements, such as the level of the Creatinine phosphokinase (CPK) enzyme in the blood, the number of platelets, etc.\n",
    "* The class (target) variable is encoded as a binary (boolean) death event: `1` if the patient died during the follow-up period, `0` if the patient did not die during the follow-up period.\n",
    "\n",
    "We'll load this dataset as a [DataFrame instance](https://dataframes.juliadata.org/stable/) and store it in the `originaldataset::DataFrame` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa87b0d9-6eb2-428f-95ec-0ff35c461323",
   "metadata": {},
   "outputs": [],
   "source": [
    "originaldataset = MyHeartDiseaseClinicalDataset(); # load the heart disease dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41015c9a-7b7e-4bb2-859c-a8fe35cbe0f5",
   "metadata": {},
   "source": [
    "<a id=\"scaling\"></a>\n",
    "\n",
    "#### Data scaling\n",
    "Our binary classification models require [a `Matrix`](https://docs.julialang.org/en/v1/base/arrays/#Base.Matrix-Tuple{UndefInitializer,%20Any,%20Any}), not [a `DataFrame`](https://dataframes.juliadata.org/stable/). We preprocess the data in three steps:\n",
    "\n",
    "> __Data Preprocessing:__\n",
    "> \n",
    "> * __Binary recoding:__ Convert categorical `0,1` data to `-1,1` where `0` maps to `-1` and `1` remains `1`.\n",
    "> * __Z-score normalization:__ Apply [z-score scaling](https://en.wikipedia.org/wiki/Feature_scaling) to continuous features using $x^{\\prime} = (x - \\mu)/\\sigma$ where $\\mu$ is the mean and $\\sigma$ is the standard deviation.\n",
    "> * __Label retention:__ Keep the `death_event` label because the classifiers are supervised.\n",
    "\n",
    "The preprocessed feature matrix is stored in `X::Matrix{Float64}`, while the label vector is stored in `y::Vector{Float64}`. We also keep the treated dataset in `dataset::DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd46d13-6580-4513-84a1-dfa25cb2ab9e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "(X, y, dataset) = let\n",
    "\n",
    "    # convert 0,1 into -1,1\n",
    "    treated_dataset = copy(originaldataset);\n",
    "    transform!(treated_dataset, :anaemia => ByRow(x -> (x==0 ? -1 : 1)) => :anaemia); # maps anaemia to -1,1\n",
    "    transform!(treated_dataset, :diabetes => ByRow(x -> (x==0 ? -1 : 1)) => :diabetes); # maps diabetes to -1,1\n",
    "    transform!(treated_dataset, :high_blood_pressure => ByRow(x -> (x==0 ? -1 : 1)) => :high_blood_pressure); # maps high_blood_pressure to -1,1\n",
    "    transform!(treated_dataset, :sex => ByRow(x -> (x==0 ? -1 : 1)) => :sex); # maps sex to -1,1\n",
    "    transform!(treated_dataset, :smoking => ByRow(x -> (x==0 ? -1 : 1)) => :smoking); # maps smoking to -1,1\n",
    "    transform!(treated_dataset, :death_event => ByRow(x -> (x==0 ? -1 : 1)) => :death_event); # maps death_event to -1,1\n",
    "    \n",
    "    D = treated_dataset[:,1:end] |> Matrix; # build a data matrix from the DataFrame\n",
    "    (number_of_examples, number_of_features) = size(D);\n",
    "\n",
    "    # Which cols do we want to rescale?\n",
    "    index_to_z_scale = [\n",
    "        1 ; # 1 age\n",
    "        3 ; # 2 creatinine_phosphokinase\n",
    "        5 ; # 3 ejection_fraction\n",
    "        7 ; # 4 platelets\n",
    "        8 ; # 5 serum_creatinine\n",
    "        9 ; # 6 serum_sodium\n",
    "        12 ; # 7 time\n",
    "    ];\n",
    "\n",
    "    D̂ = copy(D);\n",
    "    for i ∈ eachindex(index_to_z_scale)\n",
    "        j = index_to_z_scale[i];\n",
    "        μ = mean(D[:,j]); # compute the mean\n",
    "        σ = std(D[:,j]); # compute std\n",
    "\n",
    "        # rescale -\n",
    "        for k ∈ 1:number_of_examples\n",
    "            D̂[k,j] = (D[k,j] - μ)/σ; # z-scale the continuous features\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # split features and labels\n",
    "    X = D̂[:,1:end-1]; # features\n",
    "    y = D̂[:,end]; # labels (death_event)\n",
    "\n",
    "    X, y, treated_dataset\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382dcac1",
   "metadata": {},
   "source": [
    "Now, let's form the centered data matrix $\\tilde{\\mathbf{X}}$ by subtracting the mean from each column (feature) of the data matrix $\\mathbf{X}$. We store the centered data in the `X̃::Array{Float64,2}` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e89760",
   "metadata": {},
   "outputs": [],
   "source": [
    "X̃ = let \n",
    "    r, c = size(X)\n",
    "    m = mean(X, dims=1) |> vec # mean for each dimension\n",
    "    ones_vector = ones(r)\n",
    "    # Center the data by subtracting mean from each column\n",
    "    # ⊗ (outer product) replicates the mean vector for each row\n",
    "    X̃ = X .- ⊗(ones_vector, m); # the ⊗ operator is the outer product, which replicates the mean vector for each example, allowing us to subtract it from each row of X\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4293bd",
   "metadata": {},
   "source": [
    "Finally, let's compute the empirical covariance matrix $\\hat{\\mathbf{\\Sigma}}$ and store it in the `Σ̂::Array{Float64,2}` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd2cbb4-f040-4580-85b9-1f311d184739",
   "metadata": {},
   "outputs": [],
   "source": [
    "Σ̂ = let \n",
    "\n",
    "    # initialize -\n",
    "    (r,c) = size(X̃)\n",
    "    Σ = (1/(r-1)) * (transpose(X̃) * X̃); # empirical covariance matrix\n",
    "    Σ; # return the empirical covariance matrix\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de29e32-61b6-422e-b73b-6c75f3a46e78",
   "metadata": {},
   "source": [
    "Next, split the centered dataset `X̃` into `training::NamedTuple` and `test::NamedTuple` subsets using a random 80/20 partition. Each named tuple has fields `X::Array{Float64,2}` (the feature matrix with a bias column appended) and `y::Vector{Float64}` (the labels). The `training` data will be used to estimate model parameters and the `test` data for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f4d216-30b1-4b0e-9bf2-afafda98e4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training, test = let\n",
    "    \n",
    "    # initialize -\n",
    "    s = 0.80; # fraction of data for training\n",
    "    D = X̃; # use scaled features\n",
    "    number_of_training_samples = Int(round(s * size(D,1))); # 80% of the data for training\n",
    "    i = randperm(size(D,1)); # random permutation of the indices\n",
    "    training_indices = i[1:number_of_training_samples]; # first 80% of the indices\n",
    "    testing_indices = i[number_of_training_samples+1:end]; # last 20% of the indices    \n",
    "    \n",
    "    # setup training -\n",
    "    one_vector = ones(number_of_training_samples);\n",
    "    training = (X=[D[training_indices, :] one_vector], y=y[training_indices]);\n",
    "\n",
    "    # setup testing -\n",
    "    one_vector = ones(length(testing_indices));\n",
    "    testing = (X=[D[testing_indices, :] one_vector], y=y[testing_indices]);\n",
    "    training, testing;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cd093b",
   "metadata": {},
   "source": [
    "Do some checks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1420f431",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "    println(\"Sanity checks (data shapes and labels):\")\n",
    "    println(\"X size: \", size(X), \", y length: \", length(y))\n",
    "    println(\"X̃ size: \", size(X̃))\n",
    "    println(\"Training X size: \", size(training.X), \", Training y length: \", length(training.y))\n",
    "    println(\"Test X size: \", size(test.X), \", Test y length: \", length(test.y))\n",
    "\n",
    "    label_values = sort(unique(y))\n",
    "    println(\"Label values: \", label_values)\n",
    "    if length(label_values) != 2 || !(all(v -> v in (-1.0, 1.0), label_values))\n",
    "        println(\"Warning: unexpected label values; expected -1 and 1.\")\n",
    "    end\n",
    "\n",
    "    center_error = maximum(abs.(mean(X̃, dims=1)))\n",
    "    println(\"Centering check (max abs mean of X̃): \", center_error)\n",
    "    if center_error > 1e-8\n",
    "        println(\"Warning: centered features are not near zero mean.\")\n",
    "    end\n",
    "\n",
    "    if any(isnan, X) || any(isnan, y)\n",
    "        println(\"Warning: NaNs detected in X or y.\")\n",
    "    end\n",
    "    if any(isinf, X)\n",
    "        println(\"Warning: Infs detected in X.\")\n",
    "    end\n",
    "\n",
    "    nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b77e48-93bd-426a-a12a-821069bbc5e4",
   "metadata": {},
   "source": [
    "Finally, set up the color dictionary to visualize the classification datasets. The keys of the `my_color_dictionary::Dict{Int64, RGB}` dictionary are class labels, i.e., $ y\\in\\{1,-1\\}$, while the values are the colors mapped to that label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81273fbc-a0b6-4e21-ae19-c65015a06bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_color_dictionary = Dict{Int64,RGB}();\n",
    "my_color_dictionary[1] = colorant\"#03045e\"; # color for Label = 1 (you can change color if you want!)\n",
    "my_color_dictionary[-1] = colorant\"#e36414\"; # color for Label = -1 (you can change color if you want!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f57911",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466927c3-f7e1-41c1-9079-beee7781bbdd",
   "metadata": {},
   "source": [
    "<a id=\"task1\"></a>\n",
    "\n",
    "## Task 1: Visualize the dataset using Principal Component Analysis (PCA)\n",
    "In this task, you will reduce the dimension of the scaled dataset so that we can plot it in two dimensions.\n",
    "In general, imagine that we have a set of $m$-dimensional features $\\mathbf{x}\\in\\mathbb{R}^{m}$ that we want to reduce to a new set of composite feature vectors with a smaller dimension $\\mathbf{y}\\in\\mathbb{R}^{k}$ where $k\\ll{m}$. In our case, we have a `12` feature dimension, but we want to project this into `2` dimensions to visualize the label pattern.\n",
    "\n",
    "Suppose we have a transformation matrix $\\mathbf{P}\\in\\mathbb{R}^{k\\times{m}}$ so that: $\\mathbf{y} = \\mathbf{P}\\;(\\mathbf{x} - \\bar{\\mathbf{x}})$ where $\\mathbf{y}\\in\\mathbb{R}^{k}$ is the new composite feature vector, $\\mathbf{x}\\in\\mathbb{R}^m$ is the original feature vector, and $\\bar{\\mathbf{x}}$ is the mean of the features in the (original) data.  \n",
    "\n",
    "> **Why center the data?** We subtract the mean $\\bar{\\mathbf{x}}$ because dimensionality reduction seeks directions of maximum variance in the data. If we do not center, we measure distance from the origin rather than spread around the data center. Centering ensures the reduced representation captures variation in the data.\n",
    "\n",
    "If we write $\\mathbf{P} = [\\,\\mathbf{\\phi}_1^\\top;\\dots;\\mathbf{\\phi}_k^\\top]$, then each row $\\mathbf{\\phi}_i^\\top$ extracts one component of the new composite feature vector $\\mathbf{y}$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "y_{i} = \\mathbf{\\phi}_{i}^{\\top}\\;(\\mathbf{x} - \\bar{\\mathbf{x}})\\quad{i=1,2,\\dots,k}\\quad\\forall{\\mathbf{x}\\in\\mathcal{D}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "What are these transformation vectors $\\mathbf{\\phi}_{i}^{\\top}$? \n",
    "\n",
    "In brief, the $\\mathbf{\\phi}_{i}^{\\top}$ vectors are the top-$k$ eigenvectors (those corresponding to the $k$ largest eigenvalues) of the data's covariance matrix, and this reduction procedure is known as Principal Component Analysis (PCA).\n",
    "\n",
    "Compute the eigendecomposition of the covariance matrix $\\hat{\\mathbf{\\Sigma}}$ using [the `eigen(...)` method exported by the `LinearAlgebra.jl` package](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/) and save the $k\\times m$ transformation matrix in `P::Array{Float64,2}`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bc4f63-2fbd-4b10-b2ef-a2a338d84d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = let\n",
    "    \n",
    "    # Compute the eigendecomposition of the covariance matrix -\n",
    "    F = eigen(Σ̂); # eigendecomposition of the covariance matrix\n",
    "    λ = F.values;\n",
    "    V = F.vectors;\n",
    "\n",
    "    # sort the eigenpairs by eigenvalue magnitude (largest to smallest) -\n",
    "    p = sortperm(λ, rev=true); # indices that would sort λ in descending order\n",
    "    λ = λ[p]\n",
    "    V = V[:,p]\n",
    "\n",
    "\n",
    "    # initialize -\n",
    "    k = 2; # number of principal components to keep\n",
    "    P = V[:, 1:k]; # transformation matrix with top k eigenvectors as columns\n",
    "    PT = transpose(P) |> Matrix # return\n",
    "   \n",
    "    PT # return -\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47df3ca9-d620-403a-aa21-dd01695eafa4",
   "metadata": {},
   "source": [
    "### Visualize\n",
    "Let's visualize the principal components of the data by projecting the centered data onto the eigenvectors corresponding to the largest eigenvalues. The projected data points will be stored in the `Y::Array{Float64,2}` variable, where $\\mathbf{Y} = \\tilde{\\mathbf{X}}\\mathbf{P}^{\\top}$ and $\\tilde{\\mathbf{X}}$ is the centered data matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e217057",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = (X̃ * transpose(P))  |> Matrix # use the scaled, centered data matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eded87bd",
   "metadata": {},
   "source": [
    "> __What do we expect to see?__ \n",
    "> \n",
    "> Since PCA aims to capture the directions of maximum variance in the data, we expect that the first two principal components will reveal some structure in the data. \n",
    "> \n",
    "> If the features are informative with respect to the `death_event` label, we might observe some clustering or separation between the points corresponding to different classes (i.e., patients who died vs. those who survived).\n",
    "\n",
    "So what do we see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda2a2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "\n",
    "    # initialize -\n",
    "    death_event = dataset[:, :death_event] |> Vector; # extract death_event labels\n",
    "    \n",
    "    # create scatter plot\n",
    "    scatter([Y[death_event .== 1, 1]], [Y[death_event .== 1, 2]]; color=:red, label=\"Death Event\", markersize=5)\n",
    "    scatter!([Y[death_event .== -1, 1]], [Y[death_event .== -1, 2]]; color=:navy, label=\"No Death Event\", markersize=5)\n",
    "\n",
    "    # background, and labels\n",
    "    plot!(bg=\"gray95\", background_color_outside=\"white\", framestyle = :box, fg_legend = :transparent);\n",
    "    xlabel!(\"Composite Feature 1\", fontsize=18)\n",
    "    ylabel!(\"Composite Feature 2\", fontsize=18)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04726f22-4800-4ce2-b48a-20a3da29463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_I_see_the_PCA_plot = true; # TODO: update this flag {true | false}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab92e1",
   "metadata": {},
   "source": [
    "\n",
    "> __Feature importance in PCA:__ \n",
    "> \n",
    "> The eigenvector coefficients indicate how much each of the original $m$ features contributes to a composite feature. For some eigenvector $\\mathbf{z}$, the ith scaled component $l(\\mathbf{z})_{i}$ is given by:\n",
    "> $$\n",
    "\\begin{align*}\n",
    "    l(\\mathbf{z})_{i} &= \\frac{\\text{abs}(z_{i})}{\\sum_{j=1}^{m}\\text{abs}(z_{j})}\\quad{i=1,2,\\dots,m}\n",
    "\\end{align*}    \n",
    "$$\n",
    "> The scaled vectors should sum to `1`; thus, we can think about the elements (loosely) as probabilities, i.e., the probability that the ith component is the most important. \n",
    "\n",
    "\n",
    "Compute the scaled loadings for the first principal component.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c578bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "let    \n",
    "    # initialize -\n",
    "    k = 1; # look at only the first principal component\n",
    "    ϕ = P[k,:]; # get the transformation vector\n",
    "    df = DataFrame(); # allocate some space for the data in the table\n",
    "\n",
    "    # compute the scaled loadings (abs value of the coefficients scaled by the sum)\n",
    "    numerator = abs.(ϕ);\n",
    "    denominator = sum(numerator);\n",
    "    scaled_loadings = numerator ./ denominator;\n",
    "\n",
    "    # Display feature importance\n",
    "    feature_names = names(dataset)[1:end-1]; # exclude death_event column\n",
    "\n",
    "    # compute the ordinal rankings of the scaled loadings\n",
    "    p = sortperm(scaled_loadings, rev=true)          # indices in descending order\n",
    "    r = similar(p)\n",
    "    r[p] = 1:length(scaled_loadings)       # r[i] = ordinal rank of scaled_loadings[i]\n",
    "\n",
    "    # let's populate the DataFrame so we can make a nice table -\n",
    "    for i ∈ eachindex(feature_names)\n",
    "        \n",
    "        feature = feature_names[i];\n",
    "        loading = scaled_loadings[i];\n",
    "        rank = r[i];\n",
    "        push!(df, (Feature=feature, Scaled_Loading=loading, Rank=rank));\n",
    "    end\n",
    "\n",
    "    # make a table -\n",
    "    pretty_table(\n",
    "        df;\n",
    "        backend = :text,\n",
    "        table_format = TextTableFormat(borders = text_table_borders__compact)\n",
    "    );\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce71ae44",
   "metadata": {},
   "source": [
    "Next, we move to the perceptron classifier.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabf044b",
   "metadata": {},
   "source": [
    "<a id=\"task2\"></a>\n",
    "\n",
    "## Task 2: Perceptron Classification using online learning\n",
    "In this task, we build and train a [Perceptron](https://en.wikipedia.org/wiki/Perceptron) classifier using the training data, and then challenge this classifier using the `test` dataset. \n",
    "\n",
    "> __Convergence note:__ If the dataset $\\mathcal{D}$ is linearly separable, the Perceptron converges to a separating hyperplane in finite iterations. If $\\mathcal{D}$ is not linearly separable, the Perceptron may not converge. To avoid infinite loops, we set a maximum number of mistakes $M$ (e.g., $M=1$) and a maximum number of iterations $T$.\n",
    "\n",
    "__Initialize__: Given a linearly separable dataset $\\mathcal{D} = \\left\\{(\\mathbf{x}_{1},y_{1}),\\dotsc,(\\mathbf{x}_{n},y_{n})\\right\\}$, the maximum number of iterations $T$, and the maximum number of mistakes $M$ (e.g., $M=1$), initialize the parameter vector $\\mathbf{\\theta} = \\left(\\mathbf{w}, b\\right)$ to small random values and set the loop counter $t\\gets{0}$.\n",
    "\n",
    "> **Rule of thumb for maximum iterations**: Set the maximum number of iterations $T = 10n$ to $100n$, where $n$ is the number of training examples. The algorithm often converges faster for linearly separable data. However, for non-separable data, a larger $T$ may be necessary to achieve satisfactory performance.\n",
    "\n",
    "While $\\texttt{true}$ __do__:\n",
    "1. Initialize the number of mistakes $\\texttt{mistakes} = 0$.\n",
    "2. For each training example $(\\mathbf{x}, y) \\in \\mathcal{D}$: compute $y\\;\\left(\\mathbf{\\theta}^{\\top}\\;\\mathbf{x}\\right)\\leq{0}$. \n",
    "    - If $\\texttt{true}$: the example is __misclassified__ (the sign of the prediction doesn't match the label $y$). Update $\\mathbf{\\theta} \\gets \\mathbf{\\theta} + y\\;\\mathbf{x}$ and increment $\\texttt{mistakes} \\gets \\texttt{mistakes} + 1$.\n",
    "3. After processing all examples, if $\\texttt{mistakes} \\leq {M}$ or $t \\geq T$, exit. Otherwise, increment $t \\gets t + 1$ and repeat from step 1.\n",
    "\n",
    "We aim to minimize mistakes, with $M = 0$ being ideal. However, zero mistakes may not be achievable for weakly separable or non-separable data.\n",
    "\n",
    "__Training__: Our Perceptron implementation stores problem data in a [MyPerceptronClassificationModel instance](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/types/#VLDataScienceMachineLearningPackage.MyPerceptronClassificationModel). We learn parameters using the [learn(...) method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/binaryclassification/#VLDataScienceMachineLearningPackage.learn), which takes the feature array `X`, labels vector `y`, and problem instance, returning an updated instance with the learned parameters. The trained classifier is stored in `model_perceptron::MyPerceptronClassificationModel`.\n",
    "\n",
    "Train the perceptron model on the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9dbf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_perceptron = let\n",
    "\n",
    "    # data -\n",
    "    X = training.X; # input matrix\n",
    "    y = training.y; # output vector\n",
    "    number_of_examples = size(X,1); # how many examples do we have (rows)\n",
    "    number_of_features = size(X,2); # how many features do we have (cols)?\n",
    "    maxiter = 100*number_of_examples; # maximum number of iterations\n",
    "\n",
    "    # model\n",
    "    model = build(MyPerceptronClassificationModel, (\n",
    "        parameters = ones(number_of_features), # initial value for the parameters: these will be updated\n",
    "        mistakes = 0 # willing to live with m mistakes\n",
    "    ));\n",
    "\n",
    "    # train -\n",
    "    model = learn(X,y,model, maxiter = maxiter, verbose = true);\n",
    "\n",
    "    # return -\n",
    "    model;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b9ca2c",
   "metadata": {},
   "source": [
    "Now we evaluate the trained model on unseen test data.\n",
    "\n",
    "> __Inference__ \n",
    "> \n",
    "> We run classification on test data using the [classify(...) method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/binaryclassification/#VLDataScienceMachineLearningPackage.classify). This takes the feature array `X` and trained model, returning estimated labels. We store actual labels in `y_perceptron::Vector{Float64}` and predicted labels in `ŷ_perceptron::Vector{Float64}`.\n",
    "\n",
    "Run inference on the test set to obtain predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3582b6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ_perceptron,y_perceptron = let\n",
    "\n",
    "    X = test.X; # what dataset are going to use?\n",
    "    y = test.y; # what are the actual labels?\n",
    "    number_of_examples = size(X,1); # how many examples do we have (rows)\n",
    "    number_of_features = size(X,2); # how many features do we have (cols)?\n",
    "\n",
    "    # compute the estimated labels -\n",
    "    ŷ = classify(X,model_perceptron);\n",
    "\n",
    "    # return -\n",
    "    ŷ,y\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e426fd",
   "metadata": {},
   "source": [
    "Before computing the full confusion matrix, we tally the raw number of misclassified test examples in `number_of_prediction_mistakes::Int64` and print the overall mistake percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214f6245",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_prediction_mistakes = let\n",
    "\n",
    "    # initialize -\n",
    "    number_of_test_examples = length(ŷ_perceptron);\n",
    "    error_counter = 0;\n",
    "\n",
    "    for i ∈ 1:number_of_test_examples\n",
    "        if (ŷ_perceptron[i] != y_perceptron[i])\n",
    "            error_counter += 1;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # print the number of mistakes -\n",
    "    println(\"Number of Perceptron mistakes: $(error_counter) of $(length(ŷ_perceptron)) test examples.\")\n",
    "\n",
    "    error_counter # return the number of prediction mistakes\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41b960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Perceptron mistake percentage: $((number_of_prediction_mistakes/length(ŷ_perceptron))*100)%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6e0579",
   "metadata": {},
   "source": [
    "Beyond the raw mistake count, we evaluate classifier performance using [the confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix). The confusion matrix compares predicted labels $\\hat{y}_{i}$ to actual labels $y_{i}$ and categorizes each prediction as a true positive, true negative, false positive, or false negative.\n",
    "\n",
    "> **Error analysis using the confusion matrix**\n",
    ">\n",
    "> Total mistakes alone do not reveal *where* the classifier fails. In a clinical setting, we need to distinguish: How often did we predict death when the patient survived (false positive)? How often did we predict survival when the patient died (false negative)?\n",
    ">\n",
    "> The confusion matrix for a binary classifier:\n",
    ">\n",
    ">|                     | **Predicted Positive** | **Predicted Negative** |\n",
    ">|---------------------|------------------------|------------------------|\n",
    ">| **Actual Positive** | True Positive (TP)     | False Negative (FN)    |\n",
    ">| **Actual Negative** | False Positive (FP)    | True Negative (TN)     |\n",
    ">\n",
    "> From these counts we derive three metrics:\n",
    ">\n",
    "> * **Accuracy** is the fraction of correct predictions: $\\texttt{accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$. It can mislead on imbalanced data; a classifier that always predicts \"survived\" could score high if most patients survived.\n",
    ">\n",
    "> * **Precision** answers \"when we predict death, how often are we right?\" $\\texttt{precision} = \\frac{TP}{TP + FP}$. High precision means fewer false alarms.\n",
    ">\n",
    "> * **Recall** (sensitivity) answers \"of all patients who died, how many did we catch?\" $\\texttt{recall} = \\frac{TP}{TP + FN}$. In diagnosis, recall is often more critical; missing a death is worse than a false alarm.\n",
    "\n",
    "Compute the confusion matrix and store it in `CM_perceptron::Array{Int64,2}`, then extract `accuracy_perceptron::Float64`, `precision_perceptron::Float64`, and `recall_perceptron::Float64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a417c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "CM_perceptron = confusion(y_perceptron,ŷ_perceptron) # important: actual labels first, estimated labels second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eacffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_perceptron, precision_perceptron, recall_perceptron = let\n",
    "\n",
    "    # extract confusion matrix values -\n",
    "    TP = CM_perceptron[1,1]; # true positives\n",
    "    TN = CM_perceptron[2,2]; # true negatives\n",
    "    FP = CM_perceptron[2,1]; # false positives\n",
    "    FN = CM_perceptron[1,2]; # false negatives\n",
    "\n",
    "    # compute metrics -\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN); # accuracy\n",
    "    precision = TP / (TP + FP); # precision\n",
    "    recall = TP / (TP + FN); # recall\n",
    "\n",
    "    # print so the user can see -\n",
    "    println(\"Perceptron Accuracy: $(round(accuracy*100,digits=2))%\")\n",
    "    println(\"Perceptron Precision: $(round(precision*100,digits=2))%\")\n",
    "    println(\"Perceptron Recall: $(round(recall*100,digits=2))%\")\n",
    "\n",
    "    # return -\n",
    "    accuracy, precision, recall\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47792c5",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c598b7-ec09-4903-8026-c9310702797f",
   "metadata": {},
   "source": [
    "<a id=\"task3\"></a>\n",
    "\n",
    "## Task 3: Logistic Regression using Gradient Descent\n",
    "In this task, we build and train a [Logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) classifier using the training data, and then challenge this classifier using the `test` dataset. We'll use [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) to minimize the negative log-likelihood loss function. A [pseudocode outline of our training algorithm can be found here](docs/CHEME-5820-Algorithm-Simplified-GD-Spring-2026.ipynb).\n",
    "\n",
    "We implemented [the `MyLogisticRegressionClassificationModel` type](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/types/), which contains data required to solve the logistic regression problem, i.e., parameters, the learning rate, a stopping tolerance parameter $\\epsilon$, and a loss (objective) function that we want to minimize.\n",
    "\n",
    "Convergence uses the tolerance $\\epsilon$ on parameter change: stop when $\\|\\theta^{(t)}-\\theta^{(t-1)}\\|_{2} \\le \\epsilon$.\n",
    "\n",
    "> __Details__\n",
    "> \n",
    "> * __Technical note__: In this implementation, we approximate the gradient using a forward finite difference.\n",
    "> * __Note on the loss function__: In the code below, we use the natural logarithm `log` in the loss function. You could also use `log10`. While this differs from the mathematical derivation above (which uses natural log), it does not change the location of the minimum since `log10` is simply a scaled version of the natural log. The gradient descent algorithm will find the same optimal parameters $\\theta$.\n",
    "> * In the code block below, we [build a `model::MyLogisticRegressionClassificationModel` instance using a `build(...)` method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/types/). The model instance initially has a random guess for the classifier parameters. We use gradient descent to refine that guess [using the `learn(...)` method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/binaryclassification/), which returns an updated model instance (with the best parameters that we found so far). \n",
    "\n",
    "We return the updated model instance and save it in the `model_logistic_gd::MyLogisticRegressionClassificationModel` variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7047099c-2637-4a03-b000-f10c306a772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logistic_gd = let\n",
    "\n",
    "    # data -\n",
    "    X = training.X; # feature matrix\n",
    "    y = training.y; # labels\n",
    "    number_of_features = size(X,2); # number of features + 1\n",
    "    T = 1.0; # inverse temperature (called β in the math) for logistic regression\n",
    "    h = 1e-6; # step size for finite difference gradient approximation\n",
    "    λ = 0.0; # regularization parameter\n",
    "    ϵ = 1e-6; # tolerance for convergence\n",
    "\n",
    "    # model\n",
    "    model = build(MyLogisticRegressionClassificationModel, (\n",
    "        parameters = 0.01*ones(number_of_features), # initial value for the parameters: these will be updated\n",
    "        learning_rate = 0.005, # you pick this\n",
    "        ϵ = ϵ, # you pick this (this is the tolerance for convergence)\n",
    "        h = h, # you pick this (this is the step size for finite difference gradient approximation)\n",
    "        λ = λ, # regularization parameter\n",
    "        T = T, # inverse temperature (called β in the math) for logistic regression\n",
    "        loss_function = (x,y,T,λ,θ) -> log(1+exp(-2*y*T*(dot(x,θ)))) + λ*norm(θ,2)^2 # cross-entropy loss with L2 regularization\n",
    "    ));\n",
    "\n",
    "    # train -\n",
    "    model = learn(X,y,model, maxiter = 20000, verbose = true); # this is learning the model parameters\n",
    "\n",
    "    # return -\n",
    "    model;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94596661-c975-4804-8cf1-78d7b51a206d",
   "metadata": {},
   "source": [
    "Now we test how well the trained `model_logistic_gd::MyLogisticRegressionClassificationModel` instance classifies data it has never seen, i.e., the `test` dataset.\n",
    "\n",
    "> __Inference__: We run classification on the test data [using the `classify(...)` method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/binaryclassification/). Unlike the Perceptron, logistic regression returns class probabilities: each row of the output matrix is a test instance and each column corresponds to a label (`1` or `-1`). We convert these to hard predictions by selecting the highest-probability class.\n",
    "\n",
    "We store the predicted labels in `ŷ_logistic_gd::Vector{Float64}`, the actual labels in `y_logistic_gd::Vector{Float64}`, and the raw class probabilities in `probability_matrix::Array{Float64,2}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5c92b1-ff26-4b4f-8d52-6acf4f9471ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ_logistic_gd,y_logistic_gd, probability_matrix = let\n",
    "\n",
    "    D = test.X; # What dataset are you going to use?\n",
    "    y = test.y; # what are the actual labels?\n",
    "    number_of_examples = size(D,1); # how many examples do we have (rows)\n",
    "    number_of_features = size(D,2); # how many features do we have (cols)?\n",
    "\n",
    "    # compute the estimated labels -\n",
    "    P = classify(D, model_logistic_gd) # logistic regression returns an n x 2 array holding the probability\n",
    "\n",
    "    # convert the probability to a choice ... for each row (test instance), compute the col with the highest probability\n",
    "    ŷ = zeros(number_of_examples);\n",
    "    for i ∈ 1:number_of_examples\n",
    "        a = argmax(P[i,:]); # col index with largest value\n",
    "        ŷ[i] = 1; # default\n",
    "        if (a == 2)\n",
    "            ŷ[i] = -1;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # return -\n",
    "    ŷ, y, P\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7667f8cb-c8cc-4142-9347-18a9df24375f",
   "metadata": {},
   "source": [
    "We evaluate the logistic regression classifier using the same confusion-matrix framework introduced in Task 2. Although logistic regression outputs class probabilities rather than hard labels, we converted these to predictions in the previous cell by selecting the highest-probability class.\n",
    "\n",
    "Compute the confusion matrix and store it in `CM_logistic_gd::Array{Int64,2}`, then extract `accuracy_logistic::Float64`, `precision_logistic::Float64`, and `recall_logistic::Float64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0211cb-f382-4a5b-98b8-9660125027ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "CM_logistic_gd = confusion(y_logistic_gd, ŷ_logistic_gd) # actual, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11d4289",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_logistic, precision_logistic, recall_logistic = let\n",
    "\n",
    "    # extract confusion matrix values -\n",
    "    TP = CM_logistic_gd[1,1]; # true positives\n",
    "    TN = CM_logistic_gd[2,2]; # true negatives\n",
    "    FP = CM_logistic_gd[2,1]; # false positives\n",
    "    FN = CM_logistic_gd[1,2]; # false negatives\n",
    "\n",
    "    # compute metrics -\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN); # accuracy\n",
    "    precision = TP / (TP + FP); # precision\n",
    "    recall = TP / (TP + FN); # recall\n",
    "\n",
    "    # print so the user can see -\n",
    "    println(\"Logistic Regression Accuracy: $(round(accuracy*100,digits=2))%\")\n",
    "    println(\"Logistic Regression Precision: $(round(precision*100,digits=2))%\")\n",
    "    println(\"Logistic Regression Recall: $(round(recall*100,digits=2))%\")\n",
    "\n",
    "    # return -\n",
    "    accuracy, precision, recall\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d8e9fe",
   "metadata": {},
   "source": [
    "What does `probability_matrix::Array{Float64,2}` look like? If the classification is confident, probabilities will be close to 0 or 1; if uncertain, they will cluster near 0.5. The first column corresponds to the probability of class `1` and the second column to class `-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e257e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe103ece",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a4a90f-0a0a-416b-aba2-56f1638efee4",
   "metadata": {},
   "source": [
    "<a id=\"discussion\"></a>\n",
    "\n",
    "## Discussion\n",
    "We now have two trained classifiers for the same clinical prediction task. The tables below compare their confusion-matrix entries and learned parameters side by side. Use these comparisons to answer the discussion questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8817d513-e999-4e36-81d3-0bd9608db96e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "let\n",
    "    df = DataFrame();\n",
    "\n",
    "    # GD -\n",
    "    push!(df, (\n",
    "        label = \"Logistic Regression (GD)\",\n",
    "        TP = CM_logistic_gd[1,1],\n",
    "        FN = CM_logistic_gd[1,2],\n",
    "        FP = CM_logistic_gd[2,1],\n",
    "        TN = CM_logistic_gd[2,2],\n",
    "        Accuracy = round(accuracy_logistic, digits=4),\n",
    "        Precision = round(precision_logistic, digits=4),\n",
    "        Recall = round(recall_logistic, digits=4)\n",
    "    ))\n",
    "\n",
    "    # Perceptron -\n",
    "    push!(df, (\n",
    "        label = \"Perceptron\",\n",
    "        TP = CM_perceptron[1,1],\n",
    "        FN = CM_perceptron[1,2],\n",
    "        FP = CM_perceptron[2,1],\n",
    "        TN = CM_perceptron[2,2],\n",
    "        Accuracy = round(accuracy_perceptron, digits=4),\n",
    "        Precision = round(precision_perceptron, digits=4),\n",
    "        Recall = round(recall_perceptron, digits=4)\n",
    "    ))\n",
    "\n",
    "    # make a table -\n",
    "    pretty_table(\n",
    "        df;\n",
    "        backend = :text,\n",
    "        fit_table_in_display_horizontally = false,\n",
    "        table_format = TextTableFormat(borders = text_table_borders__compact)\n",
    "    );\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea0f6d6-6bfc-4771-8e2c-df066395ba54",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_I_see_confusion_table = true; # TODO: update this flag {true | false}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e0a33d-c55c-4877-8a1e-eff50fbe48df",
   "metadata": {},
   "source": [
    "Next, compare the parameter values estimated by the perceptron and logistic regression models. The `abs_p_diff` field denotes the absolute percentage difference from the perceptron parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fa0282-555a-464c-ad5d-1abbbefd78f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "    df = DataFrame();\n",
    "    θ₁ = model_perceptron.β;\n",
    "    θ₂ = model_logistic_gd.β;\n",
    "    number_of_parameters = length(θ₁);\n",
    "\n",
    "    for i ∈ 1:number_of_parameters\n",
    "        row_df = (\n",
    "            parameter_index = i,\n",
    "            Perceptron = θ₁[i],\n",
    "            Logistic = θ₂[i],\n",
    "            abs_p_diff = 100*abs((θ₁[i] - θ₂[i])/θ₁[i])\n",
    "        );\n",
    "        push!(df, row_df);\n",
    "    end\n",
    "\n",
    "    pretty_table(\n",
    "        df;\n",
    "        backend = :text,\n",
    "        table_format = TextTableFormat(borders = text_table_borders__compact)\n",
    "    );\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a326d2b1-16cb-4aea-a4f5-4bdbb24ff837",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_I_see_parameter_table = true; # TODO: update this flag {true | false}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b3892d-9b46-489d-b826-39b02addec2d",
   "metadata": {},
   "source": [
    "**DQ1: Which classifier would you deploy for mortality screening?** The comparison table above shows confusion-matrix entries and metrics for both models on the same test split. In a clinical setting, missing a patient who will die (false negative) and flagging a healthy patient for urgent follow-up (false positive) carry very different costs.\n",
    "\n",
    "> __Strategy__: Using the accuracy, precision, and recall values from the table, state which classifier you would choose for this dataset. Justify your choice in 2–3 sentences, explicitly addressing whether you prioritize precision or recall for a mortality screening task and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7032fae9-f44a-4e99-8abf-3bf236ab1b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer DQ1 here after running the models and recording metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caaffed-9b28-4a17-bcec-32f46b572462",
   "metadata": {},
   "outputs": [],
   "source": [
    "did_I_answer_DQ1 = true; # TODO: update to true if you answered DQ1 {true | false}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d63d246-4a70-43b0-9ad0-3c8cbb026cc9",
   "metadata": {},
   "source": [
    "**DQ2: How does the inverse temperature $\\beta$ affect clinical predictions?** In the logistic regression loss function, the inverse temperature (called `T` in the code) controls how sharply the model separates classes. A small `T` produces soft, uncertain probabilities; a large `T` pushes probabilities toward 0 or 1.\n",
    "\n",
    "> __Strategy:__ Change `T` in the logistic regression training cell to at least three values (e.g., $2^{-2}$, $2^{0}$, $2^{2}$), re-run the training and inference cells each time, and record the accuracy, precision, and recall. How do these metrics shift as `T` increases? Which value of `T` would you choose for this clinical task, and does the answer depend on whether you prioritize catching all deaths (recall) versus avoiding false alarms (precision)? Explain in 2–3 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2670fc17-8135-44f1-ba54-dd4f017d8e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer DQ2 here after sweeping `T` and recording metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228b857e-e29b-46ee-a743-de239e1f59fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "did_I_answer_DQ2 = true; # TODO: update to true if you answered DQ2 {true | false}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0125c7-3ac4-472b-bdfb-3a3e17cf810f",
   "metadata": {},
   "source": [
    "**DQ3: How does regularization $\\lambda$ affect the learned weights and test performance?** The L2 regularization term $\\lambda\\|\\boldsymbol{\\theta}\\|_{2}^{2}$ penalizes large weights, trading off training-set fit against generalization. Over-regularizing can force the model to ignore risk factors that genuinely predict mortality.\n",
    "\n",
    "> __Strategy:__ Reset `T` to `1.0`, then change `λ` in the logistic regression training cell to at least three values (e.g., $0$, $10^{-2}$, $10^{-1}$). Re-run training and check both the test metrics and the parameter comparison table. How do the weight magnitudes and classification metrics change as $\\lambda$ increases? At what point does regularization start to hurt test performance, and what does that tell you about the complexity needed for this dataset? Explain in 2–3 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ea825d-d4f0-4cde-803a-05c10e0e10cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer DQ3 here after sweeping `\\lambda` and recording metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd0ff35-41ab-4599-908a-c2406399553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "did_I_answer_DQ3 = true; # TODO: update to true if you answered DQ3 {true | false}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e90201",
   "metadata": {},
   "source": [
    "**DQ4: Do PCA, the perceptron, and logistic regression agree on which clinical features matter?** The PCA loadings table in Task 1 ranks features by their contribution to the first principal component. The parameter comparison table above shows the learned weights for both classifiers. Different methods can emphasize different features depending on their objective; PCA maximizes variance, while the classifiers minimize prediction error.\n",
    "\n",
    "> __Strategy:__ Compare the top-3 features identified by PCA loadings, the perceptron weights, and the logistic regression weights (use absolute values for a fair comparison). Where do the rankings agree, and where do they diverge? [Chicco and Jurman (2020)](https://pubmed.ncbi.nlm.nih.gov/32013925/) found that serum creatinine and ejection fraction alone could predict survival; do your three methods support that finding? Discuss in 3–4 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd134436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer DQ4 here after computing feature rankings across methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3220f381",
   "metadata": {},
   "outputs": [],
   "source": [
    "did_I_answer_DQ4 = true; # TODO: update to true if you answered DQ4 {true | false}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7376b1b9",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e660c37",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "\n",
    "## Summary\n",
    "We investigated whether routine clinical risk factors can predict heart-failure mortality using two linear classifiers. After preprocessing the 299-patient dataset (binary recoding and z-score normalization), PCA visualization revealed the degree of class overlap in two dimensions, and both the perceptron and logistic regression were trained and evaluated on the same 80/20 split.\n",
    "\n",
    "> __Key Takeaways:__\n",
    ">\n",
    "> * **Preprocessing determines model input quality:** Recoding binary features to $\\{-1,1\\}$ and z-score scaling continuous features places all inputs on comparable scales for gradient-based learning.\n",
    "> * **PCA reveals class separability before training:** Projecting onto the top two eigenvectors provides a visual check of whether the classes are likely separable by a linear boundary.\n",
    "> * **Confusion-matrix metrics expose different failure modes:** Accuracy alone can hide class imbalance; precision and recall reveal whether the classifier is biased toward false positives or false negatives, a critical distinction for clinical deployment.\n",
    "\n",
    "This exercise illustrates the end-to-end process of building and evaluating linear classifiers on a real clinical dataset, highlighting the importance of preprocessing, visualization, and nuanced performance metrics in machine learning for healthcare.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec5c02c-727a-4ddd-8b5c-462be7eed438",
   "metadata": {},
   "source": [
    "<a id=\"tests\"></a>\n",
    "\n",
    "## Tests\n",
    "In the code block below, we check some values in your notebook and give you feedback on which items are correct or different. `Unhide` the code block below (if you are curious) about how we implemented the tests and what we are testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9490364-804d-4a14-bb01-bd625380cf24",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "let \n",
    "    @testset verbose = true \"CHEME 5820 problem set 2 test suite\" begin\n",
    "        \n",
    "        @testset \"Setup, Prerequisites and Data\" begin\n",
    "            @test _DID_INCLUDE_FILE_GET_CALLED == true\n",
    "            @test isnothing(X) == false\n",
    "            @test isnothing(y) == false\n",
    "            @test isnothing(dataset) == false\n",
    "            @test isempty(Σ̂) == false\n",
    "            @test size(X,1) == length(y)\n",
    "            @test size(dataset,1) == size(X,1)\n",
    "            @test all(v -> v in (-1.0, 1.0), y)\n",
    "        end\n",
    "\n",
    "        @testset \"Task 1: PCA\" begin\n",
    "            @test isnothing(P) == false\n",
    "            @test isnothing(Y) == false\n",
    "            @test size(Y,1) == size(X̃,1)\n",
    "            @test size(Y,2) == 2\n",
    "            @test size(P,1) == 2\n",
    "            @test size(P,2) == size(X,2)\n",
    "            @test maximum(abs.(mean(X̃, dims=1))) < 1e-8\n",
    "            @test isapprox(Σ̂, transpose(Σ̂), atol=1e-8)\n",
    "            @test minimum(eigvals(Σ̂)) >= -1e-8\n",
    "            @test isapprox(P * transpose(P), I(2), atol=1e-6)\n",
    "            @test isapprox(Y, X̃ * transpose(P), atol=1e-8)\n",
    "            @test do_I_see_the_PCA_plot == true\n",
    "        end\n",
    "\n",
    "        @testset \"Task 2: Logistic Regression using Gradient Descent\" begin\n",
    "            @test isnothing(model_logistic_gd) == false\n",
    "            @test length(ŷ_logistic_gd) == size(test.X,1);\n",
    "            @test isnothing(CM_logistic_gd) == false\n",
    "            @test size(CM_logistic_gd,1) == 2\n",
    "            @test size(CM_logistic_gd,2) == 2\n",
    "            @test all(CM_logistic_gd .>= 0)\n",
    "            @test sum(CM_logistic_gd) == length(y_logistic_gd)\n",
    "            @test (CM_logistic_gd[1,1] + CM_logistic_gd[2,1]) > 0\n",
    "            @test (CM_logistic_gd[1,1] + CM_logistic_gd[1,2]) > 0\n",
    "            @test 0.0 <= accuracy_logistic <= 1.0\n",
    "            @test 0.0 <= precision_logistic <= 1.0\n",
    "            @test 0.0 <= recall_logistic <= 1.0\n",
    "        end\n",
    "\n",
    "        @testset \"Task 2: Perceptron\" begin\n",
    "            @test isnothing(model_perceptron) == false\n",
    "            @test length(ŷ_perceptron) == size(test.X,1);\n",
    "            @test isnothing(CM_perceptron) == false\n",
    "            @test size(CM_perceptron,1) == 2\n",
    "            @test size(CM_perceptron,2) == 2\n",
    "            @test all(CM_perceptron .>= 0)\n",
    "            @test sum(CM_perceptron) == length(y_perceptron)\n",
    "            @test (CM_perceptron[1,1] + CM_perceptron[2,1]) > 0\n",
    "            @test (CM_perceptron[1,1] + CM_perceptron[1,2]) > 0\n",
    "            @test 0.0 <= accuracy_perceptron <= 1.0\n",
    "            @test 0.0 <= precision_perceptron <= 1.0\n",
    "            @test 0.0 <= recall_perceptron <= 1.0\n",
    "        end\n",
    "\n",
    "        @testset \"Discussion questions\" begin\n",
    "            @test do_I_see_confusion_table == true\n",
    "            @test do_I_see_parameter_table == true\n",
    "            @test did_I_answer_DQ1 == true\n",
    "            @test did_I_answer_DQ2 == true\n",
    "            @test did_I_answer_DQ3 == true\n",
    "            @test did_I_answer_DQ4 == true\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2035be5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.12.4",
   "language": "julia",
   "name": "julia-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
